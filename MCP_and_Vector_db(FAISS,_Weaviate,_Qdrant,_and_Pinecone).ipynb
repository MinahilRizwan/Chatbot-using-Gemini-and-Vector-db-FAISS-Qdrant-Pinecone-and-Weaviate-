{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "FAISS"
      ],
      "metadata": {
        "id": "6saYt9iRQFtN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5p9fbIgNY-5"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu google-generativeai\n",
        "\n"
      ],
      "metadata": {
        "id": "Mz0tcJUYWZ4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure the API key directly\n",
        "genai.configure(api_key=\"Your API key here\")\n",
        "embedding_model = genai.get_model(\"models/embedding-001\")\n",
        "embedding_dim = 768\n",
        "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "vector_store_texts = []"
      ],
      "metadata": {
        "id": "QYD69cK4U71p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_to_vector_store(text):\n",
        "    embedding = embedding_model.embed(content=text)[\"embedding\"]\n",
        "    faiss_index.add(np.array([embedding], dtype='float32'))\n",
        "    vector_store_texts.append(text)\n"
      ],
      "metadata": {
        "id": "PZhX91LoXZxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_vector_store(query, top_k=3):\n",
        "    embedding = embedding_model.embed(content=query)[\"embedding\"]\n",
        "    D, I = faiss_index.search(np.array([embedding], dtype='float32'), top_k)\n",
        "    results = [vector_store_texts[i] for i in I[0] if i < len(vector_store_texts)]\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "cZyBhX7jXdr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gemini_reply(user_input):\n",
        "    relevant_contexts = search_vector_store(user_input)\n",
        "    context_text = \"\\n\".join(relevant_contexts)\n",
        "\n",
        "    prompt_with_context = f\"Context:\\n{context_text}\\n\\nUser: {user_input}\"\n",
        "\n",
        "    conversation_history.append(types.Content(role=\"user\", parts=[types.Part(text=prompt_with_context)]))\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        contents=conversation_history,\n",
        "        config=config_with_search,\n",
        "    )\n",
        "    reply_text = response.candidates[0].content.parts[0].text\n",
        "\n",
        "    # Store input and response in FAISS\n",
        "    add_to_vector_store(user_input)\n",
        "    add_to_vector_store(reply_text)\n",
        "\n",
        "    conversation_history.append(types.Content(role=\"model\", parts=[types.Part(text=reply_text)]))\n",
        "    return reply_text\n"
      ],
      "metadata": {
        "id": "gBRfVxbwXg6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import display, Markdown\n",
        "from google.generativeai.types import GenerationConfig\n",
        "\n",
        "# Configure API\n",
        "GOOGLE_API_KEY = \"Your api key here\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# Dummy vector store functions\n",
        "def search_vector_store(query):\n",
        "    return [\"Climate change refers to long-term shifts in temperatures and weather patterns.\"]\n",
        "\n",
        "def add_to_vector_store(text):\n",
        "    pass\n",
        "config_with_search = GenerationConfig(\n",
        "    temperature=0.7,\n",
        "    top_p=1,\n",
        "    top_k=1,\n",
        "    max_output_tokens=1024\n",
        ")\n",
        "\n",
        "# Simulated user input\n",
        "user_input = \"Explain the causes of climate change.\"\n",
        "relevant_contexts = search_vector_store(user_input)\n",
        "context_text = \"\\n\".join(relevant_contexts)\n",
        "prompt_with_context = f\"Context:\\n{context_text}\\n\\nUser: {user_input}\"\n",
        "\n",
        "# Build history with plain dicts\n",
        "conversation_history = []\n",
        "conversation_history.append({\n",
        "    \"role\": \"user\",\n",
        "    \"parts\": [prompt_with_context]\n",
        "})\n",
        "\n",
        "# Call Gemini\n",
        "try:\n",
        "    response = model.generate_content(\n",
        "        contents=conversation_history,\n",
        "        generation_config=config_with_search\n",
        "    )\n",
        "    reply_text = response.text\n",
        "    display(Markdown(reply_text))\n",
        "\n",
        "    # Store in vector DB\n",
        "    add_to_vector_store(user_input)\n",
        "    add_to_vector_store(reply_text)\n",
        "\n",
        "    # Add model response to history\n",
        "    conversation_history.append({\n",
        "        \"role\": \"model\",\n",
        "        \"parts\": [reply_text]\n",
        "    })\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)\n"
      ],
      "metadata": {
        "id": "olkBmPHxZdTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_gemini(user_input):\n",
        "    # 1. Search context\n",
        "    relevant_contexts = search_vector_store(user_input)\n",
        "    context_text = \"\\n\".join(relevant_contexts)\n",
        "    prompt_with_context = f\"Context:\\n{context_text}\\n\\nUser: {user_input}\"\n",
        "    conversation_history.append(\n",
        "        Content(role=\"user\", parts=[Part(text=prompt_with_context)])\n",
        "    )\n",
        "    response = model.generate_content(\n",
        "        contents=conversation_history,\n",
        "        generation_config=generation_config\n",
        "    )\n",
        "    reply_text = response.text\n",
        "    display(Markdown(f\"**Gemini:** {reply_text}\"))\n",
        "\n",
        "    add_to_vector_store(user_input)\n",
        "    add_to_vector_store(reply_text)\n",
        "\n",
        "    # 6. Append model response to history\n",
        "    conversation_history.append(\n",
        "        Content(role=\"model\", parts=[Part(text=reply_text)])\n",
        "    )\n"
      ],
      "metadata": {
        "id": "fXYhoc42bu2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector db and MCP Chatbot"
      ],
      "metadata": {
        "id": "k8PalzkJgVu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Install requirements\n",
        "!pip install -q google-generativeai faiss-cpu\n",
        "\n",
        "# === Imports ===\n",
        "import google.generativeai as genai\n",
        "import faiss\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# === API Setup ===\n",
        "GOOGLE_API_KEY = \"Your api key here\"  # Replace with your actual API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# === Vector Store Setup ===\n",
        "embedding_dimension = 768\n",
        "index = faiss.IndexFlatIP(embedding_dimension)\n",
        "texts = []\n",
        "\n",
        "def get_embedding(text):\n",
        "    res = genai.embed_content(\n",
        "        model=\"models/embedding-001\",\n",
        "        content=text,\n",
        "        task_type=\"retrieval_document\"\n",
        "    )\n",
        "    emb = np.array(res['embedding'], dtype=\"float32\")\n",
        "    return emb / np.linalg.norm(emb)\n",
        "\n",
        "def add_to_vector_store(text):\n",
        "    emb = get_embedding(text)\n",
        "    index.add(np.expand_dims(emb, axis=0))\n",
        "    texts.append(text)\n",
        "\n",
        "def search_vector_store(query, top_k=3):\n",
        "    if index.ntotal == 0:\n",
        "        return []\n",
        "    emb = get_embedding(query)\n",
        "    D, I = index.search(np.expand_dims(emb, axis=0), top_k)\n",
        "    return [texts[i] for i in I[0] if i < len(texts)]\n",
        "\n",
        "# === Chat Setup ===\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "generation_config = {\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_output_tokens\": 500,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "\n",
        "conversation_history = []\n",
        "\n",
        "def chat_with_gemini(user_input):\n",
        "    retrieved = search_vector_store(user_input)\n",
        "    context_text = \"\\n\".join(retrieved)\n",
        "\n",
        "    prompt = f\"Context:\\n{context_text}\\n\\nUser: {user_input}\"\n",
        "\n",
        "    conversation_history.append({\n",
        "        \"role\": \"user\",\n",
        "        \"parts\": [{\"text\": prompt}]\n",
        "    })\n",
        "\n",
        "    response = model.generate_content(\n",
        "        contents=conversation_history,\n",
        "        generation_config=generation_config\n",
        "    )\n",
        "\n",
        "    reply_text = response.text\n",
        "\n",
        "    display(Markdown(f\"**Gemini:** {reply_text}\"))\n",
        "\n",
        "    add_to_vector_store(user_input)\n",
        "    add_to_vector_store(reply_text)\n",
        "\n",
        "    conversation_history.append({\n",
        "        \"role\": \"model\",\n",
        "        \"parts\": [{\"text\": reply_text}]\n",
        "    })\n",
        "\n",
        "# === Usage ===\n",
        "print(\"Gemini chatbot ready. Type your message (or 'exit' to quit):\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    chat_with_gemini(user_input)\n"
      ],
      "metadata": {
        "id": "SnaGV8iqcGJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PInecone"
      ],
      "metadata": {
        "id": "Sf-yNpFdqA6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai pinecone"
      ],
      "metadata": {
        "id": "FY36UEGlp_yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pinecone"
      ],
      "metadata": {
        "id": "CcYDVsqMqm7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import numpy as np\n",
        "\n",
        "# Configure Gemini\n",
        "genai.configure(api_key=\"Your API key here\")\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# Refer to Pinecone documentation for the correct initialization for ServerlessSpec\n",
        "pc = Pinecone(api_key=\"Your pinecone api key\")\n",
        "index = pc.Index(\"gemini-chat\")\n",
        "namespace = \"chat\"\n",
        "\n",
        "def embed(text):\n",
        "    return model.embed_content(\n",
        "        content=text,\n",
        "        task_type=\"retrieval_document\",\n",
        "        title=\"user input\"\n",
        "    )['embedding']\n",
        "\n",
        "# Add to vector DB\n",
        "def add_to_pinecone(text, id_):\n",
        "    vector = embed(text)\n",
        "    index.upsert(vectors=[{\"id\": id_, \"values\": vector}], namespace=namespace)\n",
        "\n",
        "# Search vector DB\n",
        "def search_pinecone(query, top_k=5):\n",
        "    vector = embed(query)\n",
        "    res = index.query(vector=vector, top_k=top_k, namespace=namespace, include_metadata=True)\n",
        "    return [match['metadata'].get('text', '') for match in res['matches']]\n",
        "\n",
        "# Chat logic\n",
        "conversation = []\n",
        "\n",
        "def chat(user_input):\n",
        "    results = search_pinecone(user_input)\n",
        "    context = \"\\n\".join(results)\n",
        "    prompt = f\"Context:\\n{context}\\n\\nUser: {user_input}\"\n",
        "    conversation.append({\"role\": \"user\", \"parts\": [prompt]})\n",
        "\n",
        "    response = model.generate_content(conversation)\n",
        "    reply = response.text\n",
        "    print(\"Gemini:\", reply)\n",
        "\n",
        "    conversation.append({\"role\": \"model\", \"parts\": [reply]})\n",
        "    add_to_pinecone(user_input, f\"user-{len(conversation)}\")\n",
        "    add_to_pinecone(reply, f\"model-{len(conversation)}\")"
      ],
      "metadata": {
        "id": "liRiRsb-oCYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qdrant"
      ],
      "metadata": {
        "id": "7BxNIrDPrR2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qdrant-client google-generativeai"
      ],
      "metadata": {
        "id": "JhVP9uzwrRUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qdrant-client google-generativeai\n"
      ],
      "metadata": {
        "id": "N8cib892JTSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "# Generate a random UUID (version 4)\n",
        "new_uuid = str(uuid.uuid4())\n",
        "\n",
        "print(new_uuid)\n"
      ],
      "metadata": {
        "id": "ftpr3o21rI25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies if needed (uncomment if running in a fresh Colab session)\n",
        "!pip install google-generativeai qdrant-client\n",
        "\n",
        "embedding_model = genai.GenerativeModel(\"embed-text-bison@001\")\n"
      ],
      "metadata": {
        "id": "Atp9e9L2NCqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed(text):\n",
        "    response = genai.embed_text(text)\n",
        "    # response is a dict with embeddings under \"embedding\"\n",
        "    return response[\"embedding\"]\n"
      ],
      "metadata": {
        "id": "cAoFVRnnNPCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
        "import uuid\n",
        "\n",
        "# Configure API key\n",
        "genai.configure(api_key=\"your api key here\")\n",
        "\n",
        "# Chat model\n",
        "chat_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# Qdrant setup\n",
        "qdrant = QdrantClient(\":memory:\")\n",
        "\n",
        "if not qdrant.collection_exists(\"chat\"):\n",
        "    qdrant.create_collection(\n",
        "        collection_name=\"chat\",\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE)  # Use 768 or 512 depending on embedding size; test to confirm\n",
        "    )\n",
        "\n",
        "# Embedding helper - use top-level embed_content function\n",
        "def embed(text):\n",
        "    # Use genai.embed_content instead of genai.embed_text\n",
        "    response = genai.embed_content(\n",
        "        model=\"models/embedding-001\",  # Specify the embedding model\n",
        "        content=text,\n",
        "        task_type=\"retrieval_document\" # Specify the task type\n",
        "    )\n",
        "    return response[\"embedding\"]\n",
        "\n",
        "def add_to_qdrant(text):\n",
        "    vector = embed(text)\n",
        "    unique_id = str(uuid.uuid4())\n",
        "    qdrant.upsert(\n",
        "        collection_name=\"chat\",\n",
        "        points=[PointStruct(id=unique_id, vector=vector, payload={\"text\": text})]\n",
        "    )\n",
        "\n",
        "def search_qdrant(query, top_k=5):\n",
        "    vector = embed(query)\n",
        "    hits = qdrant.search(collection_name=\"chat\", query_vector=vector, limit=top_k)\n",
        "    return [hit.payload[\"text\"] for hit in hits]\n",
        "\n",
        "conversation = []\n",
        "\n",
        "def chat(user_input):\n",
        "    results = search_qdrant(user_input)\n",
        "    context = \"\\n\".join(results)\n",
        "    prompt = f\"Context:\\n{context}\\n\\nUser: {user_input}\"\n",
        "    conversation.append({\"role\": \"user\", \"parts\": [prompt]})\n",
        "\n",
        "    response = chat_model.generate_content(conversation)\n",
        "    reply = response.text\n",
        "    print(\"Gemini:\", reply)\n",
        "\n",
        "    conversation.append({\"role\": \"model\", \"parts\": [reply]})\n",
        "    add_to_qdrant(user_input)\n",
        "    add_to_qdrant(reply)\n",
        "\n",
        "# Run loop\n",
        "print(\"ðŸ¤– Gemini Chatbot (type 'exit' to quit)\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    chat(user_input)"
      ],
      "metadata": {
        "id": "7rZfvvhOL8SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weaviate"
      ],
      "metadata": {
        "id": "uSRVCFL6rgKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install weaviate-client --upgrade\n",
        "!pip install google-generativeai\n"
      ],
      "metadata": {
        "id": "oO6Zz9xjrk-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade weaviate-client\n"
      ],
      "metadata": {
        "id": "LktPPQDOwh92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required libraries\n",
        "!pip install -q google-generativeai weaviate-client\n",
        "\n",
        "# STEP 2: Import modules\n",
        "import google.generativeai as genai\n",
        "import weaviate\n",
        "from weaviate.classes.config import Configure, DataType\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# STEP 3: Gemini API Key setup\n",
        "genai.configure(api_key=\"your api key here\")\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# STEP 4: Connect to Weaviate Cloud (make sure Weaviate project is set up at https://console.weaviate.io)\n",
        "WEAVIATE_URL = \"place your weaviate url\"  # e.g., \"abc1234.weaviate.network\"\n",
        "WEAVIATE_API_KEY = \"place weaviate api key\"\n",
        "\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=WEAVIATE_URL,\n",
        "    auth_credentials=weaviate.auth.AuthApiKey(WEAVIATE_API_KEY),\n",
        "    headers={\"X-OpenAI-Api-Key\": WEAVIATE_API_KEY}\n",
        ")\n",
        "\n",
        "# STEP 5: Create collection for chat memory if not exists\n",
        "if \"ChatMemory\" not in client.collections.list_all():\n",
        "    client.collections.create(\n",
        "        name=\"ChatMemory\",\n",
        "        properties=[{\"name\": \"text\", \"dataType\": DataType.TEXT}],\n",
        "        vectorizer_config=Configure.Vectorizer.none()  # No built-in vectorizer\n",
        "    )\n",
        "\n",
        "collection = client.collections.get(\"ChatMemory\")\n",
        "# STEP 6: Embedding function using Gemini\n",
        "def embed(text):\n",
        "    # Call genai.embed_content directly, not on the model object\n",
        "    res = genai.embed_content(\n",
        "        model=\"models/embedding-001\", # Specify the embedding model\n",
        "        content=text,\n",
        "        task_type=\"RETRIEVAL_DOCUMENT\"\n",
        "    )\n",
        "    return np.array(res[\"embedding\"], dtype=np.float32)\n",
        "\n",
        "# STEP 7: Add text to Weaviate memory\n",
        "def add_to_memory(text):\n",
        "    vector = embed(text)\n",
        "    collection.data.insert(properties={\"text\": text}, vector=vector)\n",
        "\n",
        "# STEP 8: Search Weaviate memory for relevant context\n",
        "def search_memory(query, top_k=3):\n",
        "    vector = embed(query)\n",
        "    # Corrected call to near_vector: pass vector as positional argument\n",
        "    results = collection.query.near_vector(vector, limit=top_k)\n",
        "    return [o.properties[\"text\"] for o in results.objects]\n",
        "\n",
        "# STEP 9: Conversational chatbot function\n",
        "def chat(user_input, history=[]):\n",
        "    context = search_memory(user_input)\n",
        "    context_str = \"\\n\".join(context)\n",
        "\n",
        "    prompt = f\"\"\"You are a helpful assistant. Use the following context to answer the user's question.\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "User: {user_input}\n",
        "\"\"\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    reply = response.text\n",
        "\n",
        "    # Store user input and model reply\n",
        "    add_to_memory(user_input)\n",
        "    add_to_memory(reply)\n",
        "\n",
        "    history.append((user_input, reply))\n",
        "    return reply\n",
        "\n",
        "# STEP 10: Run chatbot loop\n",
        "print(\"ðŸ¤– Gemini Chatbot (type 'exit' to quit)\\n\")\n",
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    response = chat(user_input, chat_history)\n",
        "    print(f\"Assistant: {response}\\n\")"
      ],
      "metadata": {
        "id": "YzEE2cycFNfC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}